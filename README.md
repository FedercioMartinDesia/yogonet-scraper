## üì∞ Yogonet Scraper ‚Äì IA + Python + BigQuery + Railway

Este proyecto automatiza el scraping de noticias de Yogonet utilizando un enfoque inteligente basado en IA (OpenAI GPT-4o) para identificar din√°micamente los campos clave de cada nota (**T√≠tulo, Kicker, Imagen, Link**), procesa la informaci√≥n y la carga en una tabla de Google BigQuery.

- **Deploy de referencia:** Railway (evita problemas de facturaci√≥n de GCP)
- **Arquitectura desacoplada:** todo corre en Docker, se puede adaptar a Cloud Run.

---

## üìÇ Estructura del proyecto

```plaintext
yogonet-scraper/
‚îÇ
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ main.py              # Punto de entrada, orquesta todo
‚îÇ   ‚îú‚îÄ‚îÄ scraper.py           # Scraping IA-driven con OpenAI
‚îÇ   ‚îú‚îÄ‚îÄ processor.py         # Procesamiento de datos
‚îÇ   ‚îú‚îÄ‚îÄ bq_loader.py         # Inserci√≥n en BigQuery
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt         # Dependencias Python
‚îú‚îÄ‚îÄ Dockerfile               # Imagen Docker para Railway/Cloud Run
‚îú‚îÄ‚îÄ README.md                # (Este archivo)
```
---

### üöÄ C√≥mo ejecutar el scraper

#### 1. Prerrequisitos

Para poner en marcha este proyecto, necesitar√°s lo siguiente:

* Una cuenta de Google Cloud con un proyecto y una tabla de BigQuery ya creada.
* Credenciales de Service Account de Google Cloud en formato JSON.
* Una API Key de OpenAI (GPT-4o).
* Una cuenta en Railway (es gratis, no requiere tarjeta de cr√©dito).

#### 2. Configuraci√≥n de variables de entorno

Debes configurar dos variables de entorno para que el proyecto funcione:

* **`GCP_CREDS_JSON`**: El contenido completo de tu archivo JSON de credenciales de Google Cloud (Service Account).
* **`OPENAI_API_KEY`**: Tu API Key de OpenAI.
* **`ANTHROPIC_API_KEY`**: Tu API Key de Claude.
  
Estas variables se cargan en la configuraci√≥n de Railway.

<img width="829" height="228" alt="image" src="https://github.com/user-attachments/assets/cc8041c9-7ae4-440c-b2ae-a582f3c375ea" />
<img width="824" height="234" alt="image" src="https://github.com/user-attachments/assets/64176f47-c706-4bb5-a1d9-592014520749" />
<img width="799" height="213" alt="image" src="https://github.com/user-attachments/assets/661b1e16-e173-4634-8d12-9eab70ba6bd2" />
<img width="466" height="59" alt="image" src="https://github.com/user-attachments/assets/5d4cf209-91e2-494c-96dd-c5843e8ee552" />


#### 3. Despliegue autom√°tico en Railway

1.  Sube este repositorio a **GitHub**.
2.  En Railway, haz clic en **New Project** y luego en **Deploy from GitHub Repo**.
3.  Selecciona tu repositorio. Railway detectar√° el **Dockerfile** y har√° el _build_ de forma autom√°tica.
4.  Configura las variables de entorno en **Settings > Variables**. Pega el contenido de tu JSON en `GCP_CREDS_JSON` y tu clave de OpenAI en `OPENAI_API_KEY`.
5.  Railway har√° el _deploy_ autom√°ticamente. El _script_ se ejecutar√° y cargar√° los datos en BigQuery. Puedes verificar el progreso y los resultados en la opci√≥n **View logs** del _dashboard_.

#### 4. Ejecuci√≥n local

Si prefieres ejecutar el _scraper_ en tu m√°quina local, sigue estos pasos:

1.  Instala las dependencias de Python:
    ```bash
    pip install -r requirements.txt
    ```
2.  Exporta las variables de entorno en tu terminal:
    ```bash
    export GCP_CREDS_JSON="$(cat /ruta/a/credenciales.json)"
    export OPENAI_API_KEY="sk-...."
    ```
3.  Corre el _script_ principal:
    ```bash
    python app/main.py
    ```

#### 5. Despliegue en Cloud Run

Este proyecto es 100% compatible con Google Cloud Run. Simplemente sube la imagen Docker a Google Artifact Registry o Docker Hub y despli√©gala en Cloud Run. Deber√°s configurar las variables de entorno `GCP_CREDS_JSON` y `OPENAI_API_KEY` en el servicio de Cloud Run, de la misma manera que lo har√≠as en Railway.

```plaintext
#!/bin/bash
docker build -t gcr.io/tu-proyecto/tu-imagen .
docker push gcr.io/tu-proyecto/tu-imagen
gcloud run deploy tu-servicio --image gcr.io/tu-proyecto/tu-imagen --platform managed --region us-central1
```




---
### ¬øC√≥mo cambiar la URL de la noticia a scrapear?

Por defecto, la URL de la noticia a scrapear est√° definida en app/main.py en la variable news_url.
Puedes editar ese valor antes de correr el script para probar diferentes noticias de Yogonet.

Si deseas hacer el scraper interactivo, reemplaza la l√≠nea:
```
news_url = "https://www.yogonet.com/international/news/....."

```

---

### ‚öôÔ∏è Dependencias clave

El proyecto se basa en las siguientes librer√≠as de Python:

| Paquete                  | Descripci√≥n                                          |
|--------------------------|------------------------------------------------------|
| `requests`               | Descarga de HTML desde la web.                       |
| `beautifulsoup4`         | Parseo y extracci√≥n de datos de HTML.                |
| `openai`                 | Interacci√≥n con GPT-4o para extracci√≥n din√°mica.     |
| `pandas`                 | Procesamiento y transformaci√≥n de datos.             |
| `pyarrow`                | Serializaci√≥n de DataFrames y compatibilidad con BQ. |
| `pandas-gbq`             | Carga de DataFrames directamente en BigQuery.        |
| `google-cloud-bigquery`  | Cliente oficial para gesti√≥n de tablas en BigQuery.  |

---
### Modelo

El c√≥digo utiliza el modelo de OpenAI "gpt-4o" y Claude "claude-2.1".

---

### Error 429 "You exceeded your current quota, please check your plan and billing details".

Esto sucede si excediste la cuota de Openai o Claude, debes mejorar el plan.








